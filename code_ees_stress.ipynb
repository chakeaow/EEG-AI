{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chakeaow/EEG-AI/blob/main/code_ees_stress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUz5MH0hBVFu"
      },
      "source": [
        "## Dataset from Kaggle (Cradits)\n",
        "Collaborators : prashantgehlot2404  \\\n",
        "you can download dataset from this link : [stress_detection.csv](https://www.kaggle.com/datasets/prashantgehlot2404/eeg-dataset-stress-detection) or this link [Dataset_moreDetail](https://figshare.com/articles/dataset/SAM_40_Dataset_of_40_Subject_EEG_Recordings_to_Monitor_the_Induced-Stress_while_performing_Stroop_Color-Word_Test_Arithmetic_Task_and_Mirror_Image_Recognition_Task/14562090/1?file=27956376) \\\n",
        "Here link is paper of this data where you can see more info : [paper](https://www.sciencedirect.com/science/article/pii/S2352340921010465/pdfft?md5=a06ef90ca4125c8a91555aee4e05cef3&pid=1-s2.0-S2352340921010465-main.pdf)\n",
        "\n",
        "*Ethical approval was obtained from the Institutional Ethics Committee, Gauhati University,reference no. GUIEC/2019/019 dated: 15/10/2019. The experiment involved human subjects in research whose participation was completely consensual, anonymous, and voluntary. Before opting to partake in the study, the participants were informed about the nature of the study. The data collection was conducted according to the Declaration of Helsinki.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck-B5s_3Hmb6"
      },
      "source": [
        "#### Down load file lable and eeg wave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCbWxsllFyHB"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y unrar\n",
        "!pip install rarfile\n",
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHxEguX5BfhR"
      },
      "outputs": [],
      "source": [
        "! wget https://figshare.com/ndownloader/files/27956376"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6NjnyV-G-BM"
      },
      "outputs": [],
      "source": [
        "import rarfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz9KtrAeGr-G"
      },
      "outputs": [],
      "source": [
        "rar_path = '27956376'\n",
        "extract_path = './extracted_files'\n",
        "\n",
        "with rarfile.RarFile(rar_path) as rf:\n",
        "    rf.extractall(extract_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sry1f_r4H9TS"
      },
      "source": [
        "### perprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBl1ebmgIILp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import mne # eeg libery\n",
        "\n",
        "from scipy.io import loadmat # to load matlab file\n",
        "import glob\n",
        "import os\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ1ImHC_Hz2Q"
      },
      "outputs": [],
      "source": [
        "## function to pre-prpcess\n",
        "\n",
        "def Download_file(folder_path,mat_files,i) :\n",
        "    file_path = os.path.join(folder_path, mat_files[i])\n",
        "    data = loadmat(file_path)\n",
        "    sample_data = data.copy() #dic key that contain data is Clean_data\n",
        "    raw_data = sample_data['Clean_data'] #loading\n",
        "    return (raw_data)\n",
        "\n",
        "def create_INFO (raw_data) :\n",
        "\n",
        "    n_channel, n_time = raw_data.shape #find number of chan , range of f\n",
        "    #create info : info = mne.create_info(channel_names, sampling_rate, channel_types)\n",
        "\n",
        "    info = mne.create_info(\n",
        "    ch_names = chan_list ,\n",
        "    sfreq = f,\n",
        "    ch_types = [\"eeg\"]* n_channel )\n",
        "    info.set_montage('standard_1020') #add node location\n",
        "\n",
        "\n",
        "    raw = mne.io.RawArray(raw_data, info) #create data for mne\n",
        "    #raw.plot(n_channels=12, duration=6, scalings= \"auto\")\n",
        "    return(raw)\n",
        "\n",
        "def Re_reference (raw) :\n",
        "    raw_reref = raw.copy().set_eeg_reference(ref_channels=\"average\", projection=True)  # projection by average\n",
        "    #print(\"projection by average plot :\")\n",
        "    #raw_processed.plot(n_channels=12, duration=6, scalings= \"auto\")\n",
        "    #raw_processed.apply_proj()\n",
        "    return (raw_reref)\n",
        "\n",
        "def filtered(raw_reref) :\n",
        "    raw_fillter = raw_reref .copy().filter(l_freq= 0.5 , h_freq= 45)\n",
        "    #raw_fillter.plot(n_channels=12, duration=6, scalings= \"auto\")\n",
        "    #raw_fillter.pick(picks='eeg').compute_psd(fmax=raw_processed.info['sfreq']/2).plot()\n",
        "    return (raw_fillter)\n",
        "\n",
        "def ica(raw_fillter):\n",
        "    ica = mne.preprocessing.ICA(n_components=32, random_state=97, max_iter='auto')\n",
        "    ica.fit(raw_fillter)\n",
        "    ica.plot_sources(raw_fillter, show_scrollbars=False)  #EEG wave\n",
        "    ica.plot_components() #Topographic\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty2DVEXgIqUp"
      },
      "source": [
        "Clean Lables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ez1ukvuIV5P"
      },
      "outputs": [],
      "source": [
        "#downlodd file\n",
        "file_xls = pd.read_excel('/content/extracted_files/Data/scales.xls')\n",
        "print(file_xls.head(),\"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubGW7dgfI3Bp"
      },
      "outputs": [],
      "source": [
        "file_xls.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erQLhi8NJY1W"
      },
      "outputs": [],
      "source": [
        "newColumname = ['Subject No.', \"ArithmeticT1\", 'MirrorT1', 'StroopT1', \"ArithmeticT2\",\"MirrorT2\", 'StroopT2', 'ArithmeticT3', 'MirrorT3', 'StroopT3']\n",
        "file_xls.columns =  newColumname\n",
        "file_xls.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFMlB5dtKuLr"
      },
      "outputs": [],
      "source": [
        "file_xls = file_xls.drop(0)\n",
        "file_xls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILS9IQyxIosi"
      },
      "outputs": [],
      "source": [
        "scell = {}\n",
        "sub = file_xls[newColumname[0]]\n",
        "#colect allevent lables\n",
        "Ari1,Ari2,Ari3 = file_xls[newColumname[1]], file_xls[newColumname[4]], file_xls[newColumname[7]]\n",
        "stro1,stro2,stro3 = file_xls[newColumname[3]], file_xls[newColumname[6]], file_xls[newColumname[9]]\n",
        "mir1,mir2,mir3 = file_xls[newColumname[2]], file_xls[newColumname[5]], file_xls[newColumname[8]]\n",
        "\n",
        "for i in sub :\n",
        "    n = int(i)\n",
        "    score = [Ari1[n],Ari2[n]]\n",
        "    a = \"Arithmetic\" + str(n) + 't_'\n",
        "    scell[a+\"1\"] = Ari1[n]; scell[a+\"2\"] = Ari2[n]; scell[a+\"3\"] = Ari3[n]\n",
        "\n",
        "    s = \"Stroop\" + str(n) + 't_'\n",
        "    scell[s+'1'] = stro1[n]; scell[s+'2'] = stro2[n]; scell[s+'3'] = stro3[n]\n",
        "\n",
        "    m = \"Mirror\" + str(n) + 't_'\n",
        "    scell[m+'1'] = mir1[n]; scell[m+'2'] = mir2[n]; scell[m+'3'] = mir3[n]\n",
        "\n",
        "    r = \"Relax\" + str(n) + 't_'\n",
        "    scell[r+'1'],scell[r+'2'],scell[r+'3'] = 1,1,1\n",
        "\n",
        "print(scell,\"\\n\",type(scell))\n",
        "\n",
        "###---------------- convert in to 3 level -----------------# (you can change in to two level here)\n",
        "\n",
        "def scell_to_level(x):\n",
        "    if x <= 4 : return -1 #negative\n",
        "    elif x <= 6: return 0 #normal\n",
        "    return 1 # positive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE-CniJPN3K0"
      },
      "outputs": [],
      "source": [
        "count = [0,0,0,0,0,0,0,0,0,0]\n",
        "for i in scell.values() :\n",
        "    count[i-1] += 1\n",
        "\n",
        "print(\"Density of rate :\",count)\n",
        "print(\"Total data :\",sum(count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av7kMzguOA0t"
      },
      "source": [
        "perprocess data (in actuly it's already preprocess by we will redo in agann for sure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl8U9UjSIOGV"
      },
      "outputs": [],
      "source": [
        "# data from paper page 5\n",
        "chan_list = ['Cz', 'Fz', 'Fp1', 'F7', 'F3', 'FC1', 'C3', 'FC5', 'FT9', 'T7', 'CP5', 'CP1', 'P3', 'P7', 'PO9', 'O1', 'Pz', 'Oz', 'O2', 'PO10', 'P8', 'P4', 'CP2', 'CP6', 'T8', 'FT10', 'FC6', 'C4', 'FC2', 'F4', 'F8', 'Fp2']\n",
        "f = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXQ8SbpoOgsv"
      },
      "outputs": [],
      "source": [
        "folder_path = \"/content/extracted_files/Data/filtered_data\"\n",
        "mat_files = [f for f in os.listdir(folder_path) if f.endswith('.mat')] #find all .mat file\n",
        "print(\"we have :\",len(mat_files),'file')\n",
        "print(mat_files,\"\\n\\n\")\n",
        "\n",
        "data,event = [],[]\n",
        "label = []\n",
        "start= 0\n",
        "#task of this paper\n",
        "event_dict = {\n",
        "     'Relax' : 1,\n",
        "     'Stroop' : 2,\n",
        "     'Mirror' : 3,\n",
        "     'Arithmetic' : 4\n",
        "     }\n",
        "filter = ['Arithmetic','Mirror','Stroop', 'Relax']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ls3Fm4yLOtof"
      },
      "outputs": [],
      "source": [
        "for i in range (0,len(mat_files)):\n",
        "     name = mat_files[i].split(\"_\")\n",
        "     name.remove(\"sub\")\n",
        "     if \"image\" in name : name.remove(\"image\")\n",
        "     if name[0] not in  filter : #you can play or fillter each task in here\n",
        "          continue\n",
        "     event_name = name[0] ; trial = name[-1][-5] ; sub = name[1] #cataaorice each data cy file name\n",
        "     key = event_name + sub + \"t_\" + trial\n",
        "     #print(key)\n",
        "\n",
        "     ##--------------------- preprocess ----------------------##\n",
        "     raw_data = Download_file(folder_path,mat_files,i)\n",
        "     raw = create_INFO(raw_data)\n",
        "     raw_rr = Re_reference (raw)\n",
        "     raw_ft = filtered(raw_rr)\n",
        "     #ica(raw_ft)\n",
        "\n",
        "     ##--------------------- ephoching event manage ----------------------##\n",
        "     event_id = event_dict.get(event_name)\n",
        "     revious_value = 0\n",
        "     events = np.array([start,revious_value,event_id])\n",
        "     start += 3200\n",
        "\n",
        "     event += [events]\n",
        "     data += [raw_ft]\n",
        "\n",
        "     ##_______ match lable and task __________#\n",
        "     if event_id not in [1]: # if you want to analite but dont want to use as ml data filter in here\n",
        "          label += [ scell_to_level(scell[key]) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13Qv0SuaRZsc"
      },
      "outputs": [],
      "source": [
        "event = np.array(event)\n",
        "print(event)\n",
        "print(data)\n",
        "data = mne.concatenate_raws(data)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYpKy8b1XQni"
      },
      "outputs": [],
      "source": [
        "print(data.info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG1CYd6QXdrp"
      },
      "outputs": [],
      "source": [
        "ica(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuOQjGPyXVIJ",
        "outputId": "2888dbf4-a27e-43d0-cba5-79a7b00f7679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not setting metadata\n",
            "480 matching events found\n",
            "Applying baseline correction (mode: mean)\n",
            "Created an SSP operator (subspace dimension = 1)\n",
            "1 projection items activated\n",
            "Using data from preloaded Raw for 480 events and 641 original time points ...\n",
            "0 bad epochs dropped\n",
            "((), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), ())\n"
          ]
        }
      ],
      "source": [
        "data_epochs = mne.Epochs(data, event, event_dict, tmin=11, tmax=16, baseline=(11,11.5), event_repeated= 'merge', preload=True)\n",
        "print(data_epochs.drop_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vusfMomGXs3L"
      },
      "outputs": [],
      "source": [
        "print(data_epochs.info)\n",
        "print(\"Over all case\")\n",
        "data_epochs.average().plot_topomap(times=[12, 12.5, 13, 15])\n",
        "print(\"In relax task\")\n",
        "relax_data = data_epochs['Relax']\n",
        "relax_data.average().plot_topomap(times=[12, 12.5, 13, 15])\n",
        "\n",
        "print(\"in Stroop task\")\n",
        "stroop_data = data_epochs['Stroop']\n",
        "stroop_data.average().plot_topomap(times=[12, 12.5, 13, 15])\n",
        "\n",
        "print(\"in Mirror task\")\n",
        "mirror_data = data_epochs['Mirror']\n",
        "mirror_data.average().plot_topomap(times=[12, 12.5, 13, 15])\n",
        "\n",
        "print(\"in Arithmetic task\")\n",
        "Arithmetic_data = data_epochs['Arithmetic']\n",
        "Arithmetic_data.average().plot_topomap(times=[12, 12.5, 13, 15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUbzZkcZXxx9"
      },
      "outputs": [],
      "source": [
        "epochs = data_epochs['Stroop','Mirror', 'Arithmetic']\n",
        "epochs.info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA-puCx6X6Vh"
      },
      "source": [
        "# Analysis parth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Uwf8VR5X12g"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZLdzISAYA0Z"
      },
      "outputs": [],
      "source": [
        "#dimentional reduction\n",
        "data = epochs.get_data()\n",
        "n_epochs, n_channels, n_times = data.shape\n",
        "data = data.reshape(n_epochs, n_channels * n_times)\n",
        "\n",
        "labels = np.array(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XAqC79YGxZ"
      },
      "source": [
        "### split data to training sat and testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bG-aRnWYDL2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "\n",
        "print(data.shape,labels.shape)\n",
        "#split data to trian and test\n",
        "train_data,test_data,train_labels,test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKRHOdE0YLVH"
      },
      "outputs": [],
      "source": [
        "print(train_data,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfpGFpjnYUQ6"
      },
      "source": [
        "### Synthetic Minority Over-sampling\n",
        "if you don't wanna use Over-sampling data do not run this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC23qBElYPLM"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdXLnyNzYg-I"
      },
      "outputs": [],
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "train_data, train_labels = smote.fit_resample(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5SlAhTiYjlm"
      },
      "outputs": [],
      "source": [
        "c = [0,0,0]\n",
        "for i in (train_labels):\n",
        "    if i == -1 :\n",
        "        c[0] += 1\n",
        "    elif i == 0 :\n",
        "        c[1] += 1\n",
        "    else :\n",
        "        c[2] += 1\n",
        "print(\"Train data lables : \",c)\n",
        "\n",
        "c = [0,0,0]\n",
        "for i in (test_labels):\n",
        "    if i == -1 :\n",
        "        c[0] += 1\n",
        "    elif i == 0 :\n",
        "        c[1] += 1\n",
        "    else :\n",
        "        c[2] += 1\n",
        "print(\"Test data lables : \",c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTWuXsyhYokf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error,confusion_matrix, ConfusionMatrixDisplay,accuracy_score,classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thSFYWR5ZBnF"
      },
      "source": [
        "## Train_RandomForest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwJLL74kYvk6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_X-hALpZKNH"
      },
      "source": [
        "### Hyperparameters junning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htN06Yh_ZJdN"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [350, 390, 420],\n",
        "    'max_depth': [10 ,15, 20],\n",
        "    'min_samples_split': [5, 11, 15],\n",
        "    'min_samples_leaf': [1],\n",
        "    'class_weight': [ 'balanced_subsample']\n",
        "} # range of paramiter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LIjKtGHzZkEJ"
      },
      "outputs": [],
      "source": [
        "rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "rf.fit(train_data, train_labels)\n",
        "print(\"Best Parameters:\", rf.best_params_)\n",
        "print(\"Best Score:\", rf.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2p5X7txmZ2Aq"
      },
      "outputs": [],
      "source": [
        "best_rf = rf.best_estimator_\n",
        "rf_pre = best_rf.predict(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CZoqSffZnac"
      },
      "source": [
        "### result\n",
        "As a result, you can see it's overfitting and failing to identify the negative class, which is my main goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1mD7bIF6ZxoS"
      },
      "outputs": [],
      "source": [
        "##confusion_matrix\n",
        "\n",
        "accuracy = accuracy_score(test_labels, rf_pre) #add train\n",
        "print(\"Accuracy :\",accuracy)\n",
        "\n",
        "print('\\nTest Set: Classification report')\n",
        "print(classification_report(test_labels, rf_pre))\n",
        "\n",
        "cm = confusion_matrix(test_labels, rf_pre)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Reds)\n",
        "plt.title('Confusion Matrix of RandomForest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLst6ArRaYiv"
      },
      "source": [
        "## K Neighbors model\n",
        "I don’t fully understand this model, so maybe you can improve it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PamN2E4UaiRO"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK13AFzFavIX"
      },
      "source": [
        "### Hyperparameters junning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "elzQ_raHak-i"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'n_neighbors': [5, 7, 9, 11],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan'],\n",
        "    'algorithm' : ['ball_tree', 'brute']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WPniVHOlayj2"
      },
      "outputs": [],
      "source": [
        "knn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid= param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "knn.fit(train_data, train_labels)\n",
        "print(\"Best Parameters:\", knn.best_params_)\n",
        "print(\"Best Score:\", knn.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gfy3mJVta1z-"
      },
      "outputs": [],
      "source": [
        "best_knn = knn.best_estimator_\n",
        "print(best_knn)\n",
        "knn_pre = best_knn.predict(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOnxYwrjbVQN"
      },
      "source": [
        "### result\n",
        "As a result, you can see it's behaving strangely, but I’m not sure why. lol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OdlD7N3ZbQpU"
      },
      "outputs": [],
      "source": [
        "\n",
        "accuracy = accuracy_score(test_labels, knn_pre)\n",
        "print(\"Accuracy :\",accuracy)\n",
        "\n",
        "print('\\nTest Set: Classification report')\n",
        "print(classification_report(test_labels, knn_pre))\n",
        "\n",
        "cm = confusion_matrix(test_labels,knn_pre)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix of KNN')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOhEAQRqb-9k"
      },
      "source": [
        " ## Gradient Boosting Classifier  model\n",
        "I think this is the best model so far. By the way, I didn’t tune the hyperparameters because it takes too long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NYvCZ75zGiqk"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TC9SbFMb99p"
      },
      "outputs": [],
      "source": [
        "gb =  GradientBoostingClassifier(\n",
        "    n_estimators =  200,\n",
        "    learning_rate =  0.1,\n",
        "    max_depth =  10,\n",
        "    min_samples_split = 2\n",
        ")\n",
        "gb.fit(train_data, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpzdEhNtcpJa"
      },
      "outputs": [],
      "source": [
        "gb_pre2 = gb.predict(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsO0D7vJcrgl"
      },
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(test_labels, gb_pre2)\n",
        "print(\"Accuracy :\",accuracy)\n",
        "\n",
        "print('\\nTest Set: Classification report')\n",
        "print(classification_report(test_labels, gb_pre2))\n",
        "\n",
        "cm = confusion_matrix(test_labels, gb_pre2)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Purples)\n",
        "plt.title('Confusion Matrix of Gradient Boosting')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-LFUhnecy6a"
      },
      "source": [
        "## Best model feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lAdMglWc3DZ"
      },
      "outputs": [],
      "source": [
        "feature_importances = gb.feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnO1g_Aoc7H1"
      },
      "outputs": [],
      "source": [
        "print(test_data.shape)\n",
        "n_channels = 32\n",
        "n_time_points = 641  # 20512/32\n",
        "\n",
        "channel_importances = np.array([feature_importances[i * n_time_points:(i + 1) * n_time_points].sum() for i in range(n_channels)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNXX9gYoc_xo"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(chan_list, channel_importances)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"EEG Channel\")\n",
        "plt.title(\"Feature Importance per EEG Channel in GradientBoostingClassifier\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeElNRk_dCi0"
      },
      "source": [
        "### Top 16 is :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viR1ZhI5dGbC"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(chan_list, channel_importances)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"EEG Channel\")\n",
        "plt.title(\"Feature Importance per EEG Channel in GradientBoostingClassifier\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eGEzsWPdKz8"
      },
      "outputs": [],
      "source": [
        "d = {}\n",
        "for i in range(len(channel_importances)):\n",
        "    d[channel_importances[i]] = chan_list[i]\n",
        "\n",
        "channel_importances.sort()\n",
        "top16 = channel_importances[15:]\n",
        "\n",
        "print(\"top 16 channels is\")\n",
        "for i in top16:\n",
        "    print(d[i],end = \" ,\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOjOByN4rDQP/iiNp0Wy5J2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}